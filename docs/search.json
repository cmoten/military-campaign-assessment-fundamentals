[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Military Assessment Fundamentals",
    "section": "",
    "text": "Preface\nMilitary operations today unfold in environments of growing complexity, strategic ambiguity, and rapid change. In this context, commanders and their staffs must not only execute plans, but continuously assess whether those plans are achieving the intended effects—and whether the effects themselves are relevant to evolving objectives.\nYet despite the centrality of assessment to military effectiveness, the practice remains uneven, often constrained by narrow definitions, limited analytic tools, and unclear linkages between operational activities and strategic ends. Traditional methods of measurement—focused largely on performance metrics—often fail to capture the deeper questions that assessment must answer: Are we doing the right things? Are we achieving the right effects? And how can we know?\nThis book, Military Assessment Fundamentals, provides some inghight to these questions. It is designed to support both military planners and analysts in building better frameworks for evaluating campaigns, operations, and strategic efforts. Grounded in joint doctrine, but informed by recent advances in decision science, systems thinking, and strategic design, this book offers a contemporary view of assessment that treats it as a core function of command—not a post hoc report.\nThroughout the chapters, you will encounter concepts such as analytic uncertainty, causal logic, and deep vs. surface uncertainty. These are not merely academic abstractions. They reflect the real-world conditions under which military decisions must be made and adapted. You will also see how assessment connects to planning, design, and adaptation, enabling a more agile and informed approach to strategic execution.\nThis book is not a manual. It is a way to think about assessment that transcends templates and checklists. Whether you are a planner, a commander, or an analyst, the goal is to equip you with the principles, language, and mindset necessary to assess what matters most.\nI hope this work contributes to a broader conversation about military assessment and helps advance both the practice and profession of command.\nLTC Cardy Moten III\nStuttgart, Germany\nAugust 2025",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html",
    "href": "01-campaign-assessment-overview.html",
    "title": "1  Military Assessment Overview",
    "section": "",
    "text": "1.1 Motivating Problem\nBoth Joint Publication 5-0 and Joint Publication 3-0 provide the following definitions for assessment:\nFrom these definitions, several key points emerge:\nHow to gain a better understanding of the analytic uncertainty inherent with military strategies, plans, and operations so that the commander and staff have a better understanding of the effectiveness of their operation or campaign (Croft 2018).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#what-we-will-learn",
    "href": "01-campaign-assessment-overview.html#what-we-will-learn",
    "title": "1  Military Assessment Overview",
    "section": "1.2 What We will learn",
    "text": "1.2 What We will learn\nInterestingly, neither publication offers a formal definition for measurement. To fill this conceptual gap, we apply Hubbard’s definition from the field of decision science: “A quantitatively expressed reduction in uncertainty based on one or more observations” (Hubbard 2014).\nThis framing aligns closely with the role assessment plays in military planning: reducing uncertainty to inform decisions about adapting or sustaining current operations.\nIn this chapter, we will explore the relationship between uncertainty and assessment and identify what types of assessment are appropriate at different levels of military operations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#assessment-as-a-prerequisite-to-adaptation",
    "href": "01-campaign-assessment-overview.html#assessment-as-a-prerequisite-to-adaptation",
    "title": "1  Military Assessment Overview",
    "section": "1.3 Assessment as a Prerequisite to Adaptation",
    "text": "1.3 Assessment as a Prerequisite to Adaptation\nCombatant Command Campaign Plans (CCPs) and country plans are dynamic, continuously evolving through cycles of implementation, evaluation, and revision. Because plans rarely unfold as anticipated, Combatant Command (CCMD) planners must extend their planning horizon annually to maintain a forward-looking posture. This dual imperative requires balancing the execution of ongoing operations with the need to plan for future contingencies.\nIn this environment, assessment becomes a prerequisite to effective adaptation. But assessment is not just about measuring performance or progress—it is fundamentally about measuring and reducing uncertainty. In military operations, uncertainty arises from the complexity of the operational environment, the unpredictability of adversaries, and the limitations of available information. By systematically observing and evaluating how operations affect outcomes, assessment helps commanders determine whether current approaches are likely to succeed—or whether they must be adjusted or abandoned.\nFollowing Hubbard’s definition, measurement is “a quantitatively expressed reduction in uncertainty based on one or more observations” (Hubbard 2014). This perspective is particularly relevant for campaign assessment: the goal is not only to track implementation but to reduce ambiguity about what works, what doesn’t, and why.\nThe following 13 propositions, summarized from contemporary scholarship (Hickey, Bradford, and Perez 2019), offer a theoretical framework for integrating uncertainty into military design and assessment:\n\nEnvironment as Obstacle Course – Military environments are composed of dynamic systems that commanders must navigate.\nEnvironment as Landscape to Change – Commanders are tasked not only with navigating but also with reshaping the environment.\nInterventions as Causal Hypotheses – Every military action implies a theory of cause and effect.\nNeed for Causal Literacy – Military professionals must be trained to craft and evaluate causal claims.\nTypology of Causal Logics – Reductive causation (structural, institutional, ideational, psychological) underpins military reasoning.\nMastering Causal Frameworks – Commanders must understand causal logics to assess and adapt effectively.\nComplexity Limits Certainty – The world’s complexity reduces the reliability of single-cause explanations.\nAssessment as Experimentation – Interventions should be approached as hypotheses tested through assessment.\nFocus on Political Ends – Military efforts must align with and support broader political objectives.\nReevaluate Measures of Effectiveness (MoEs) and Measures of Performance (MoPs) – Traditional measures of effectiveness and performance are insufficient for complex goals.\nAssessment is Difficult but Essential – Despite challenges, continuous assessment is indispensable.\nIntegration of Planning and Assessment – Planners and assessors should work collaboratively throughout the planning cycle.\n‘For the Sake Of’ vs. ‘In Order To’ Objectives – Objectives should prioritize purpose and direction over narrowly defined end-states.\n\nThese propositions highlight that uncertainty is not a problem to be eliminated, but a reality to be engaged thoughtfully. Assessment is not merely technical—it is cognitive, adaptive, and continuous. It helps leaders make decisions in complex and unpredictable environments, not by providing perfect knowledge, but by systematically reducing what is unknown.\nUltimately, campaign assessment enables the Combatant Commander (CCDR) to make strategic recommendations to the Secretary of Defense (SecDef) and broader Department of Defense (DoD) leadership. These insights guide both internal planning cycles and higher-level decisions about force structure, budgeting, and legislative requests through the Future Years Defense Program (FYDP).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#understanding-analytic-uncertainty",
    "href": "01-campaign-assessment-overview.html#understanding-analytic-uncertainty",
    "title": "1  Military Assessment Overview",
    "section": "1.4 Understanding Analytic Uncertainty",
    "text": "1.4 Understanding Analytic Uncertainty\nAccording to Croft (2018), analytic uncertainty is “a type of uncertainty that consists of five components of deep and surface uncertainty that is useful for planning and military assessments.” Table 1.1 lists those five elements, which we will discuss in detail in the following section.\n\n\n\nTable 1.1: Elements of Analytic Uncertainty\n\n\n\n\n\nDeep Uncertainty (unknowables)\nSurface Uncertainty (unknowns)\n\n\n\n\nComplexity\nFactual Uncertainty\n\n\nNon-linearity\nPrecision Uncertainty\n\n\nAdaptation\n\n\n\n\n\n\n\n\n1.4.1 Components\n\n1.4.1.1 Surface Uncertainty\nSurface uncertainty refers to uncertainty that is visible and acknowledged, but not deeply understood. It often arises from ambiguities or contradictions in observable data or misalignment between facts and interpretation. Surface uncertainty is the “tip of the iceberg” — it’s apparent, but may conceal deeper issues beneath.\nSome key characteristics of surface uncertainty are:\n\nOften linked to conflicting or unclear indicators.\nMight be recognized by decision-makers but not resolved.\nCan result from over-reliance on dashboards, over-simplified metrics, or data with unclear context.\n\nIf not properly mitigated, surface uncertainty can lead to:\n\nRisk of misinterpretation: Decision-makers may prematurely resolve uncertainty by defaulting to assumptions or bias.\nPlanning fragility: Plans built on surface-level observations without deeper validation may be brittle under stress.\nCognitive bias exposure: It invites confirmation bias and motivated reasoning when stakeholders pick evidence that supports preferred narratives.\n\nAs a mitigation measure, we will then learn more about the components of surface uncertainty: factual uncertainty and precision uncertainty.\n\n1.4.1.1.1 Factual Uncertainty\nFactual uncertainty arises when information is incomplete, inaccurate, outdated, or ambiguous. It’s about not having the correct facts to begin with, and it reflects gaps in what we know or what we’re aware of. Some examples of this type of uncertainty are:\n\nMissing intelligence about an adversary’s troop movement.\nLack of updated terrain maps.\nAmbiguous language in diplomatic communication.\n\nBased on Figure 1.1, this form of uncertainty corresponds to known unknowns (things we are aware of but don’t understand) and unknown unknowns (things we are neither aware of nor understand).\n\n\n\n\n\n\nFigure 1.1: Factual Uncertainty\n\n\n\n\n\n1.4.1.1.2 Precision Uncertainty\nPrecision uncertainty refers to situations where we have quantitative information, but the information is imprecise or subject to variation. This type of uncertainty often emerges when using probabilistic models or estimates that inherently involve a degree of variability or error.\nSome examples of this type of uncertainty are:\n\nForecasting casualty estimates with wide confidence intervals.\nUsing intelligence or other staff estimate reports that assign vague probabilities (e.g., “likely”).\nPlanning based on weather predictions with large error margins.\n\nIn practice, analysts in the above situations can use some of the models depicted in Figure 1.2, but these models generally are not utilized in assessment frameworks. A way to mitigate these risks in assessment frameworks would be to use techniques like Bayesian models and Monte Carlo simulations.\n\n\n\n\n\n\nFigure 1.2: Precision Uncertainty\n\n\n\n\n\n\n1.4.1.2 Deep Uncertainty\nDeep uncertainty arises when stakeholders do not know or cannot agree on:\n\nThe system model (how things work).\nThe probability distributions for key variables.\nThe value trade-offs involved in making decisions.\n\nUnlike surface uncertainty, which is visible but unclear, deep uncertainty means the underlying mechanisms are fundamentally opaque or contested. It’s common in complex, dynamic military, geopolitical, and socio-technical systems.\n\n1.4.1.2.1 Complexity\nThe operational environment consists of many interconnected parts whose relationships are not easily decomposed or predictable. Changes in one area can cause ripple effects elsewhere. Some examples of this component are:\n\nA change in local governance affects regional security, economy, and foreign relations.\nCoalition operations where each partner’s decisions affect the others in non-obvious ways.\n\nBy understanding this component, assessment analysts will begin to understand the difficulty in tracing cause and effect, and that emergent events are common even when they are not planned for (Air Land Sea Space Application Center 2020, 16, 21).\nFigure 1.3 depicts a vital tool to mitigate the previously discussed risks, and that is systems thinking models. We will discuss these models more later in the course.\n\n\n\n\n\n\nFigure 1.3: Complexity\n\n\n\n\n\n1.4.1.2.2 Non-Linearity\nInputs and outputs are not proportional — small actions can have significant effects or vice versa. The system exhibits unpredictable or chaotic behavior at times.\nWhile it does not completely encapsulate this component, Figure 1.4 depicts a non-linear process. Other examples of this are:\n\nA minor political assassination triggers a large-scale war.\nLarge troop increases fail to shift insurgent activity.\n\nA key aspect of nonlinearity is that forecasting future events is highly error-prone, and success or failure may hinge on tipping points or threshold effects. To mitigate these risks, an assessment analyst needs to utilize methods and tools that lend themselves to range estimates and sensitivity analysis.\n\n\n\n\n\n\nFigure 1.4: Non-Linearity\n\n\n\n\n\n1.4.1.2.3 Adaptation\nActors within the system — including adversaries, allies, civilians, and even the organization itself — observe, learn, and change behavior in response to actions.\nFor instance:\n\nAn adversary adjusts tactics after observing drone strikes.\nLocal populations change cooperation patterns based on perceived legitimacy.\n\nFurthermore, the environment is dynamic and responsive, and the adage of plans not making it past first contact still holds. A key mitigation for this concept involves systems thinking models and the development of an agile assessment framework.\n\n\n\n\n1.4.2 Implications for Planning\nBy understanding the concept of surface uncertainty, assessment analysts can better define these components of uncertainty and ultimately measure them in a meaningful way. Analysts can also qualify risks and opportunities.\nSimilarly, understanding deep uncertainty at a conceptual level provides us with the means to better deal with it. According to Croft (2018):\n\nThe remedy that analysts must employ in the face of complexity, nonlinearity, and adaptation is theory. Theories, by their nature, are uncertain, and planners in early-stage operational design work must again avoid the assumption trap. By engaging with deep uncertainty, planners take on the challenge of building multiple worldviews as possible explanations of system behavior and change during conflict.\n\nUltimately, instead of relying on the process of creating and validating assumptions, assessment analysts can now leverage additional analytic tools that make the engineering of assessment frameworks based on the previous thirteen principles possible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#from-uncertainty-to-theory",
    "href": "01-campaign-assessment-overview.html#from-uncertainty-to-theory",
    "title": "1  Military Assessment Overview",
    "section": "1.5 From Uncertainty to Theory",
    "text": "1.5 From Uncertainty to Theory\nGiven the previous explanations, we can now establish additional terms of reference based on the work of Hubbard (2014) and Croft (2018):\nAnalytic: Relating to or using analysis or logical reasoning.\nUncertainty: The lack of complete certainty, that is, the existence of more than one possibility. The “true” outcome/state/result/value is not known.\nRisk: A state of uncertainty where some of the possibilities involve a loss, catastrophe, or other undesired outcome.\nOpportunity: A state of uncertainty where some of the possibilities involve a favorable advance in effect, time to achievement, or other desired outcomes if facilitated by decision, engagement, or resources.\nMeasurement: A quantitatively expressed reduction in uncertainty based on one or more observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#types-of-assessments-and-who-performs-them",
    "href": "01-campaign-assessment-overview.html#types-of-assessments-and-who-performs-them",
    "title": "1  Military Assessment Overview",
    "section": "1.6 Types of Assessments and Who Performs Them",
    "text": "1.6 Types of Assessments and Who Performs Them\nAs shown in Figure 1.5, military assessments align with the levels of war, with the labels of strategic, operational, and tactical. Per joint doctrine, “assessment should link with adjacent levels, both to provide a conduit for guidance and information” (Joint Chiefs of Staff 2022).\n\n\n\n\n\n\nFigure 1.5: Assessment Interaction\n\n\n\nWhile Figure 1.5 provides a way to visualize these interactions, we need some additional detail to delineate the types of assessments conducted at any one of the levels of warfare. @tbl-militaryAssessmentTypes displays four general kinds of assessments according to Paul and Matthews (2018).\n\n\n\nTable 1.2: Types of Military Assessments\n\n\n\n\n\n\n\n\n\nAssessment Type\nDescription\n\n\n\n\nStaff Estimates\nDoes not require an objective. Provides situational awareness about a context, environment, or audience. Different types of estimates are possible; always include qualifying terms.\n\n\nPerformance Assessments\nPerformance assessments capture the extent of implementation or execution of a program, operation, activity, or investment (OAI). These assessments are descriptive and generated by the entity that is responsible for executing the program or OAI, and inform a progress assessment.\n\n\nEffectiveness Assessments\nEffectiveness assessments measure the contribution of a program or OAI towards meeting an objective. These assessments are diagnostic and generated by the staff directorates that are responsible for planning and monitoring the programs and OAIs over time. This assessment also informs a progress assessment.\n\n\nProgress Assessments\nProgress assessments measure toward one or more objectives. Progress assessments may include multiple programs or OAIs, provided they all contribute to achieving the objective(s). These assessments are diagnostic and validated by a cross-functional process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#linking-assessments-together",
    "href": "01-campaign-assessment-overview.html#linking-assessments-together",
    "title": "1  Military Assessment Overview",
    "section": "1.7 Linking Assessments Together",
    "text": "1.7 Linking Assessments Together\nFigure 1.6 illustrates the interconnection of various assessments in a notional information flow diagram for a strategic command, such as a geographic combatant command. Here, service components provide performance assessments of OAIs, the command staff conducts effectiveness assessments of the command’s campaign, and a cross-functional team performs a progress assessment of the command’s objectives.\nUnderneath these general forms are numerous other assessments that are specific to certain joint functions or other matters, but these descriptions are suitable to describe the general assessment structure at the varying levels of warfare.\n\n\n\n\n\n\nFigure 1.6: Assessment Information Flow",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#practical-example",
    "href": "01-campaign-assessment-overview.html#practical-example",
    "title": "1  Military Assessment Overview",
    "section": "1.8 Practical Example",
    "text": "1.8 Practical Example\nTo enhance your understanding of the previously discussed concepts, we will run through a scenario based on something most Americans are familiar with: football. Below are the key stakeholders and examples of their perspectives about football operations.\n\n1.8.1 National Football League (NFL)\nOur Mission\nWe are all stewards of football. We unite people and inspire communities in the joy of the game by delivering the world’s most exciting sports and entertainment experience.\nOur Responsibility Every member of the NFL community embraces our unique leadership role in society, and assumes the trust, character, and responsibility that comes with that role. We bring fans and communities from all walks of life together to celebrate a game that is constantly evolving, balancing the authenticity of tradition with the power of innovation (National Football League https://operations.nfl.com/inside-football-ops/nfl-operations/nfl-ops-honoring-the-game/).\n\n\n\nNFL Logo\n\n\n\n\n1.8.2 NFL Team\nOUR TEAM\nUnder the leadership of Sheila Hamp, the great granddaughter of Henry Ford, the Lions have ushered in an era of rebirth focused on creating an inclusive and equitable experience for employees, partners and fans. Building off the rich and diverse history of the city, the Lions embrace transparency and value contribution from all areas of the organization. We believe in the power of a pride, and acknowledge winning together takes understanding, acceptance, and teamwork. We believe in our “we” culture and are committed to building a new tradition that Detroit and Michigan can be proud of (Detroit Lions 2025).\n\n\n\nDetroit Lions\n\n\n\n\n1.8.3 Coaches and Players\n\n“The only place that “success” comes before “work” is in the dictionary.” —Vince Lombardi\n\n\n“You can learn a line from a win and a book from a defeat.” —Paul Brown\n\n\n“When you win, nothing hurts.” —Joe Namath\n\n\n“Set your goals high, and don’t stop til you get there” – —Bo Jackson\n\n\n“To me, football is so much about mental toughness, it’s digging deep, it’s doing whatever you need to do to help a team win and that comes in a lot of shapes and forms.” —Tom Brady\n\n\n\n\nFootball Play",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#what-to-read",
    "href": "01-campaign-assessment-overview.html#what-to-read",
    "title": "1  Military Assessment Overview",
    "section": "1.9 What to Read",
    "text": "1.9 What to Read\n\nJoint Publication 5-0, Chapter VI (Need CAC)\nJoint Publication 3-0, Pages II-8 thru II-11 (Need CAC)\nATP 5-0.3, Chapter I\nDesign, Assessment, and the Causal Imperative in Complex Operations and Campaigns Paper\nUnderstanding Uncertainty: Incorporating the Unknown into Military Estimates Paper\n\n\n\n\n\nAir Land Sea Space Application Center. 2020. “Multi‑service Tactics, Techniques, and Procedures for Operation Assessment.” ATP 5‑0.3 / MCRP 5‑10.1 / NTTP 5‑01.3 / AFTTP 3‑2.87. Carlisle Barracks, PA: U.S. Department of Defense. https://www.alssa.mil/Portals/9/Documents/mttps/assessment_2020.pdf.\n\n\nCroft, Robert E. 2018. “Understanding Uncertainty: Incorporating the Unknown into Military Estimates.” U.S. Army War College. https://apps.dtic.mil/sti/pdfs/AD1069606.pdf.\n\n\nDetroit Lions. 2025. “One Pride.” https://www.detroitlions.com/about-us/careers.\n\n\nHickey, Christopher J, Robert Bradford, and Celestino Perez. 2019. “Design, Assessment, and the Causal Imperative in Complex Operations and Campaigns.” The Journal of Defense Modeling and Simulation 16 (4): 265–76.\n\n\nHubbard, Douglas W. 2014. How to Measure Anything: Finding the Value of Intangibles in Business. John Wiley & Sons.\n\n\nJoint Chiefs of Staff. 2022. Joint Publication 3-0: Joint Campaigns and Operations. Washington, D.C.: United States Department of Defense. https://jdeis.js.mil/jdeis/new_pubs/jp3_0.pdf.\n\n\n———. 2025. Joint Publication 5-0: Joint Planning. Washington, D.C.: United States Department of Defense. https://jdeis.js.mil/jdeis/new_pubs/jp5_0.pdf.\n\n\nNational Football League. https://operations.nfl.com/inside-football-ops/nfl-operations/nfl-ops-honoring-the-game/. “NFL Ops: Honoring the Game.”\n\n\nPaul, Christopher, and Miriam Matthews. 2018. “The Language of Inform, Influence, and Persuade: Assessment Lexicon and Usage Guide for U.S. European Command Efforts.” RR-2655-EUCOM. Santa  Monica, CA: RAND Corporation. https://www.rand.org/pubs/research_reports/RR2655.html.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "02-logic-models.html",
    "href": "02-logic-models.html",
    "title": "2  Logic Models",
    "section": "",
    "text": "2.1 Motivating Problem\nRecall in Section 1.4.2, the author noted that “the remedy that analysts must employ in the face of complexity, nonlinearity, and adaptation is theory.” Croft emphasizes that planners must articulate explicit assumptions and causal pathways to enable adaptive assessment and correction (2018).\nSimilarly, the Multi-Service Tactics, Techniques, and Procedures (MTTP) for Operation Assessment manual discusses the concept of theory of change and its critical role in assessment. The manual notes that assessment frameworks should be grounded in clearly stated theories of causation—explaining why and how inputs are expected to achieve desired end-states—and that these theories directly inform assessment techniques, indicators, and interpretation (Air Land Sea Space Application Center 2020, see Chapter II).\nA review of broader literature highlights a foundational guide developed by the W.K. Kellogg Foundation, which introduced the concept of logic models. In the context of operational design, a logic model functions as a theory of change, explicitly depicting the hypothesized causal pathways and assumptions that underpin a campaign, operation, or program.\nAccording to the W.K. Kellogg Foundation:\nHow to develop and implement the usage of logic models when framing a military assessment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Models</span>"
    ]
  },
  {
    "objectID": "02-logic-models.html#what-we-will-learn",
    "href": "02-logic-models.html#what-we-will-learn",
    "title": "2  Logic Models",
    "section": "2.2 What We Will Learn",
    "text": "2.2 What We Will Learn\n\nLogic Model Components\nHow to Use Logic Models in an Assessment Framework",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Models</span>"
    ]
  },
  {
    "objectID": "02-logic-models.html#logic-model-components",
    "href": "02-logic-models.html#logic-model-components",
    "title": "2  Logic Models",
    "section": "2.3 Logic Model Components",
    "text": "2.3 Logic Model Components\nA logic model visually and systematically represents how a program works. It links program resources, activities, outputs, and outcomes to clearly illustrate the relationships among the program’s elements and its intended results. Planners, implementers, and evaluators commonly use logic models to design, execute, assess, and communicate programs. By doing so, they promote shared understanding among stakeholders and establish accountability and clarity in measuring success.\n\n2.3.1 Inputs (Resources)\nInputs refer to the foundational resources required to launch and sustain a program. These include human resources (such as staff, volunteers, or subject-matter experts), financial resources (budgets, grants, donations), material assets (equipment, facilities, supplies), and intangible assets (community partnerships, organizational reputation, data systems). Inputs are not outputs or results themselves, but they enable the program to function. A clear understanding of inputs is crucial for determining program feasibility and scope. Inputs also serve as a baseline for accountability, ensuring that there is alignment between investments and what the program aims to accomplish. The quality, availability, and appropriateness of these resources will significantly affect the program’s effectiveness and the plausibility of achieving desired outcomes. In short, inputs are the “raw materials” for program implementation.\n\n\n2.3.2 Activities\nProgram staff carry out activities as the core processes or interventions using available inputs. These activities consist of the actual operations or services delivered to achieve the program’s objectives—such as conducting workshops, developing curricula, offering technical assistance, providing training, or running outreach campaigns. Designers typically express activities using action verbs to reflect what the program “does,” ensuring they align with the needs of the target population and the intended outcomes. These actions bridge resource use with measurable results (outputs and outcomes). To plan activities effectively, program designers must clearly understand the intervention’s logic—why they expect a particular activity to lead to change. During evaluations, analysts assess the fidelity and quality of activity implementation to determine whether the program or external factors produced the observed results.\n\n\n2.3.3 Outputs\nOutputs are the immediate, tangible products or deliverables resulting from a program’s activities. These are typically quantified and expressed in terms such as “number of participants trained,” “classes conducted,” or “manuals distributed.” Outputs do not reflect the impact or effectiveness of a program; instead, they indicate that the completion of activities as intended. Outputs are proxies for performance indicators during implementation to monitor progress and operational efficiency. While outputs alone do not demonstrate success, they are essential for program accountability and process evaluation. They also serve as a necessary precondition for achieving outcomes—no outcomes can occur without outputs, but outputs alone are not sufficient. By distinguishing between outputs and outcomes, program evaluators can avoid the common mistake of confusing effort with effect.\n\n\n2.3.4 Outcomes\nOutcomes represent the changes or benefits resulting from a program’s outputs. They can be short-term (changes in knowledge, attitudes, or skills), intermediate (changes in behavior, decision-making, or organizational practice), or long-term (changes in social, economic, or environmental conditions). Outcomes provide evidence of program effectiveness and are central to any meaningful evaluation. Unlike outputs, outcomes measure the value and impact of the program, not just completion, but achievement. Well-defined outcomes should be specific, measurable, realistic, and aligned with the program’s overall goals. They also must be attributable, at least in part, to the program’s activities and outputs. Importantly, outcomes are not always linear or guaranteed; they depend on various assumptions and external influences. Measuring outcomes helps organizations understand their value proposition and refine future strategies.\n\n\n2.3.5 Impact\nPrograms aim to contribute to broad, long-term, and often societal-level changes known as impacts. These high-level effects include reducing poverty, improving public health, increasing economic stability, and advancing educational attainment. Organizations typically align impacts with their institutional missions or policy goals, recognizing that such changes often emerge years after a program concludes. Unlike more immediate and attributable outcomes, impacts result from a complex interplay of factors, many of which lie beyond the program’s control. Consequently, evaluators rarely attribute impacts directly to a single program; instead, they recognize the program’s contribution to broader change.\nMeasuring impact requires a long-term perspective and often involves partnerships with external evaluators, the use of secondary data, or follow-up studies. In logic models, impact serves as the north star, guiding the design and alignment of all prior components: activities, outputs, and outcomes. A strong logic model will show a plausible pathway from what the program does (activities) to what it achieves (outcomes) and ultimately how it contributes to impact. Being transparent about intended impact ensures that programs remain focused on meaningful change, rather than simply delivering services. It also helps communicate value to funders, stakeholders, and the broader public.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Models</span>"
    ]
  },
  {
    "objectID": "02-logic-models.html#practical-exercise",
    "href": "02-logic-models.html#practical-exercise",
    "title": "2  Logic Models",
    "section": "2.4 Practical Exercise",
    "text": "2.4 Practical Exercise\nUsing the Detroit Lions football framework introduced in Chapter 1, create a basic logic model that maps stakeholder goals (e.g., community engagement or player development) to inputs, activities, outputs, outcomes, and potential impact.\n\n2.4.1 Example Logic Models\n\n2.4.1.1 Health Care Provider\n\n\n\nExample Logic Model - Health Care Provider\n\n\n\n\n2.4.1.2 Military Unit Training\n\n\n\nExample Logic Model - Military Unit Training\n\n\n\n\n2.4.1.3 Civil Affairs Objective\n\n\n\nExample Logic Model - Civil Affairs Objective",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Models</span>"
    ]
  },
  {
    "objectID": "02-logic-models.html#what-to-read",
    "href": "02-logic-models.html#what-to-read",
    "title": "2  Logic Models",
    "section": "2.5 What to Read",
    "text": "2.5 What to Read\n\nATP 5-0.3, Chapter II\nW.K. Kellogg Foundation Logic Model Guide\n\n\n\n\n\nAir Land Sea Space Application Center. 2020. “Multi‑service Tactics, Techniques, and Procedures for Operation Assessment.” ATP 5‑0.3 / MCRP 5‑10.1 / NTTP 5‑01.3 / AFTTP 3‑2.87. Carlisle Barracks, PA: U.S. Department of Defense. https://www.alssa.mil/Portals/9/Documents/mttps/assessment_2020.pdf.\n\n\nCroft, Robert E. 2018. “Understanding Uncertainty: Incorporating the Unknown into Military Estimates.” U.S. Army War College. https://apps.dtic.mil/sti/pdfs/AD1069606.pdf.\n\n\nW.K. Kellogg Foundation. 2004. “Logic Model Development Guide.” Battle Creek, MI: W.K. Kellogg Foundation. https://wkkf.issuelab.org/resource/logic-model-development-guide.html.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Models</span>"
    ]
  },
  {
    "objectID": "03-futures-map.html",
    "href": "03-futures-map.html",
    "title": "3  Futures Map",
    "section": "",
    "text": "3.1 Motivating Problem\nReferring back to Section 1.4.2, the author noted that “By engaging with deep uncertainty, planners take on the challenge of building multiple worldviews as possible explanations of system behavior and change during conflict” (2018). While Croft does not delve further into how to develop multiple worldviews, there is a technique that is useful to develop and analyze future states. That technique is called Field Anomaly Relaxation and is the focus of this chapter.\nHow to develop and implement the the technique of Field Anomaly Relaxation to develop a futures map.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Futures Map</span>"
    ]
  },
  {
    "objectID": "03-futures-map.html#what-we-will-learn",
    "href": "03-futures-map.html#what-we-will-learn",
    "title": "3  Futures Map",
    "section": "3.2 What We Will Learn",
    "text": "3.2 What We Will Learn\n\nWhat Field Anomaly Relaxation Is.\nHow to Develop a Futures Map Using Field Anomaly Relaxation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Futures Map</span>"
    ]
  },
  {
    "objectID": "03-futures-map.html#what-is-field-anomaly-relaxation",
    "href": "03-futures-map.html#what-is-field-anomaly-relaxation",
    "title": "3  Futures Map",
    "section": "3.3 What is Field Anomaly Relaxation?",
    "text": "3.3 What is Field Anomaly Relaxation?\nField Anomaly Relaxation (FAR) is a scenario development methodology introduced by Russell Rhyne in the 1970s to support complex strategic and policy planning under deep uncertainty (R. F. Rhyne 1971; R. Rhyne 1995). FAR was designed not to predict specific outcomes, but rather to systematically generate and refine plausible descriptions of alternative future environments. These environments, or “contexts,” allow decision-makers to understand the conditions in which future decisions might be made. Rhyne argued that good decision-making depends not on a single forecast but on recognizing a range of coherent futures.\nFAR accomplishes this through a structured and iterative process that filters out incoherent or internally inconsistent combinations of possible future conditions, leaving behind only scenarios that are logically plausible. These combinations are referred to as configurations and represent snapshots of what the world might look like across multiple dimensions or “sectors” (e.g., politics, technology, economics, culture).\nThe method emphasizes pattern recognition, participatory logic, and the elimination of contradictions, rather than numerical modeling or statistical forecasting. It encourages participants to articulate and refine their intuitive understanding of systemic change while ensuring logical consistency across variables. This makes FAR particularly well-suited for use in strategic foresight, defense planning, and long-range policy design (R. Rhyne 1995).\nThe FAR methodology proceeds in four steps: (1) forming a view of future contexts, (2) constructing a symbolic language, (3) filtering incoherent configurations, and (4) composing coherent scenarios.\n\n\n\n\n\n\nFigure 3.1: Field Anomaly Relaxation Steps",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Futures Map</span>"
    ]
  },
  {
    "objectID": "03-futures-map.html#futures-map-development-steps",
    "href": "03-futures-map.html#futures-map-development-steps",
    "title": "3  Futures Map",
    "section": "3.4 Futures Map Development Steps",
    "text": "3.4 Futures Map Development Steps\n\n3.4.1 Step 1: Form a View of the Future\nThe first step in FAR involves developing a broad and imaginative view of what the future might look like in a given field or domain (R. F. Rhyne 1971). This is an exploratory process that calls on participants to articulate their implicit and explicit expectations about how key trends might unfold. Rather than starting with numerical forecasts, FAR encourages planners to begin with rough mental models or high-level narratives of plausible futures. The aim is to stimulate insight and creativity in thinking about possible worlds.\nParticipants identify “fields” of interest—broad systems such as national security or urban development—and sketch alternative configurations that might represent future states of that field. Importantly, participants are encouraged to think structurally rather than temporally, focusing on how elements of the system relate rather than on predicting timelines.\nThis step also helps uncover deeply held assumptions and prepares participants to build a formal language to describe configurations precisely. By beginning with open-ended exploration, FAR ensures that scenario building remains grounded while maintaining the flexibility to evolve.\nStep 1 of the FAR process closely aligns with the Army Design Methodology (ADM), particularly the step of framing the operational environment as shown in Figure 3.2. In this step, planners identify relevant systems, actors, relationships, and dynamics that characterize the current context. ADM emphasizes understanding complexity and interdependence across political, military, economic, social, informational, and infrastructure domains.\nBoth methodologies aim to uncover the fundamental structure of a complex problem before proposing solutions. FAR achieves this by identifying and categorizing dimensions that drive system behavior, while ADM does so through collaborative analysis and visualization tools like system maps and operational narratives. In both cases, the outcome is a more structured understanding of the problem that informs future exploration—whether through scenario generation in FAR or problem framing in ADM. Thus, FAR’s Step 1 complements ADM by offering a rigorous and disciplined technique for dimension identification, particularly when exploring alternative futures or dealing with deep uncertainty.\n\n\n\n\n\n\nFigure 3.2: Field Anomaly Relaxation Step 1\n\n\n\n\n\n3.4.2 Step 2: Define a Symbolic Language\nOnce participants articulate initial visions of the future, they construct a symbolic language that adds structure and precision to those visions. This language consists of two key elements: sectors (broad categories like governance, technology) and factors (specific states within those sectors) (R. Rhyne 1995). Each configuration of the future is expressed as a combination of sector-factor pairs.\nFor example, a technology sector might have factors such as “high automation,” “AI dominance,” or “manual labor resurgence.” A governance sector might include “centralized control” or “decentralized autonomy.” This symbolic matrix enables planners to encode possible futures and identify logical dependencies among variables.\nThis structure allows participants to explore how combinations of factors interact across domains and whether they cohere into viable worlds. By creating a shared grammar, the symbolic language enhances communication, logic tracking, and transparency in scenario development (R. F. Rhyne 1971).\n\n\n\n\n\n\nFigure 3.3: Field Anomaly Relaxation Step 2\n\n\n\n\n\n3.4.3 Step 3: Test Internal Consistencies\nThe most distinctive step in FAR is the systematic filtering of logically inconsistent or implausible configurations. Filtering occurs in two stages: pairwise factor analysis and whole-pattern coherence checking (R. Rhyne 1995).\nIn pairwise analysis, participants assess whether two factors from different sectors can logically coexist. For example, can “anarchic governance” coexist with “highly regulated markets”? If not, that pair is excluded from all further configurations.\nWhole-pattern coherence checking then evaluates entire configurations for logical consistency and intuitive plausibility. This step relies on expert judgment and group deliberation rather than mathematical models. The process reduces a vast set of combinations to a manageable number of internally consistent, strategic futures suitable for planning.\n\n\n\n\n\n\nFigure 3.4: Field Anomaly Relaxation Step 3\n\n\n\n\n\n3.4.4 Step 4: Compose Coherent Scenarios\nIn the final step, participants translate filtered configurations into rich, detailed scenarios. Each scenario describes a coherent future world, including causal explanations, key developments, and implications for strategy (R. Rhyne 1995). These narratives incorporate multiple dimensions (e.g., economic, political, environmental) and may use timelines or dramatizations to improve understanding.\nUnlike predictive forecasts, these scenarios remain exploratory and policy-neutral, intended to serve as decision contexts. They help decision-makers explore “what-if” questions, anticipate disruptions, and design resilient strategies. The inclusion of time horizons and thematic grouping further supports actionable insight.\nThrough this process, FAR produces a small number of deeply coherent futures that organizations can use in wargaming, foresight analysis, or long-term planning.\n\n\n\n\n\n\nFigure 3.5: Field Anomaly Relaxation Step 4",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Futures Map</span>"
    ]
  },
  {
    "objectID": "03-futures-map.html#practical-exercise-building-a-futures-map-using-field-anomaly-relaxation",
    "href": "03-futures-map.html#practical-exercise-building-a-futures-map-using-field-anomaly-relaxation",
    "title": "3  Futures Map",
    "section": "3.5 Practical Exercise: Building a Futures Map Using Field Anomaly Relaxation",
    "text": "3.5 Practical Exercise: Building a Futures Map Using Field Anomaly Relaxation\nScenario: Imagine you are a strategic planning team at U.S. Africa Command (USAFRICOM) tasked with exploring future environments for military engagement across the African continent in the year 2040. Your mission is to anticipate plausible strategic contexts that could affect campaign design, posture planning, and security cooperation.\nYou will apply the Field Anomaly Relaxation (FAR) method—referred to here as Futures Mapping—to build a set of internally consistent futures that support strategic foresight.\n⸻\nInstructions:\n\nForm a View of the Future\n\nIdentify four sectors relevant to AFRICOM’s future operating environment (e.g., governance, technological diffusion, security cooperation, socio-economic development).\nFor each sector, brainstorm three plausible future states (factors).\nExample: Sector: Governance | Factors: “Stable democratic institutions”, “Hybrid regimes”, “Autocratic resurgence”.\n\nDefine a Symbolic Language\n\nCreate a matrix of your four sectors and their respective factors.\nAssign shorthand symbols to each factor (e.g., G1 = Stable Democracy, T3 = AI Dominance).\n\nTest for Internal Consistency\n\nConduct pairwise analysis to identify any incompatible factor pairs.\nEliminate inconsistent combinations and record your rationale.\n\nCompose Coherent Scenarios\n\nFrom the remaining valid configurations, select two combinations to develop into rich narrative scenarios.\nDescribe the characteristics of each world, key events that led to it, and its strategic implications for AFRICOM.\n\n\n⸻\nDeliverable: Submit a table of sectors/factors, a visual diagram of your futures map (optional), and two ~200-word narrative scenarios that reflect coherent, plausible futures.\nObjective: Apply structured logic to generate future scenarios that support adaptive planning and assessment under deep uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Futures Map</span>"
    ]
  },
  {
    "objectID": "03-futures-map.html#what-to-read",
    "href": "03-futures-map.html#what-to-read",
    "title": "3  Futures Map",
    "section": "3.6 What to Read",
    "text": "3.6 What to Read\n\nATP 5-0.3, Chapter II\nField Anomaly Relaxation: The Art of Usage\nArmy Design Methodology, Chapter 2\nMaking Information Operations Effects-Based: Begin with the End (-State) in Mind\n\n\n\n\n\nCroft, Robert E. 2018. “Understanding Uncertainty: Incorporating the Unknown into Military Estimates.” U.S. Army War College. https://apps.dtic.mil/sti/pdfs/AD1069606.pdf.\n\n\nRhyne, Russell. 1995. “Field Anomaly Relaxation: The Arts of Usage.” Futures 27 (6): 657–74.\n\n\nRhyne, Russell F. 1971. “Projecting Whole–Body Future Patterns–the Field Anomaly Relaxation (FAR) Method. Memorandum Report.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Futures Map</span>"
    ]
  },
  {
    "objectID": "04-strategic-questions.html",
    "href": "04-strategic-questions.html",
    "title": "4  Strategic Questions",
    "section": "",
    "text": "4.1 Motivating Problem\nCroft in Section 1.4.2, states “By engaging with deep uncertainty, planners take on the challenge of building multiple worldviews as possible explanations of system behavior and change during conflict” (2018). In order to provide possible explanations, planners and analysts will need to develop the right questions. For this chapter we will focus on what Hill and Gerras defined as strategic questions (2020).\nHow to develop strategic questions that will drive data collection requirements.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Strategic Questions</span>"
    ]
  },
  {
    "objectID": "04-strategic-questions.html#what-we-will-learn",
    "href": "04-strategic-questions.html#what-we-will-learn",
    "title": "4  Strategic Questions",
    "section": "4.2 What We Will Learn",
    "text": "4.2 What We Will Learn\n\nWhat strategic questions are an why they are important.\nTypes of strategic questions.\nElements of good and bad strategic questions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Strategic Questions</span>"
    ]
  },
  {
    "objectID": "04-strategic-questions.html#what-are-strategic-question-and-why-are-they-important",
    "href": "04-strategic-questions.html#what-are-strategic-question-and-why-are-they-important",
    "title": "4  Strategic Questions",
    "section": "4.3 What are Strategic Question and Why are They Important?",
    "text": "4.3 What are Strategic Question and Why are They Important?\nStrategic questions are high-level inquiries that help decision-makers understand complex problems, anticipate change, and choose among competing options in uncertain environments. Hill and Gerras argue that strategic questions are essential for guiding national security strategy, particularly when facing ambiguity, long time horizons, and limited information (2020).\nUnlike tactical or operational questions, strategic questions are deliberately open-ended. They focus on context, causality, options, and risks. Asking them enables planners to clarify assumptions, uncover blind spots, and develop better-informed courses of action. These questions support both critical and creative thinking and help national security professionals move beyond reactive thinking.\nHill and Gerras emphasize that the ability to ask good strategic questions is not innate—it requires practice and reflection. They cite the Cuban Missile Crisis as a case study where President Kennedy and his advisors asked hundreds of strategic questions over 13 tense days—such as “What happens if we do nothing?” and “What are the Soviets trying to achieve?”—which ultimately helped prevent nuclear war.\nIn short, strategic questions help planners navigate competitive uncertainty. They encourage multidimensional thinking, challenge groupthink, and promote adaptability in the face of strategic complexity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Strategic Questions</span>"
    ]
  },
  {
    "objectID": "04-strategic-questions.html#types-of-strategic-questions",
    "href": "04-strategic-questions.html#types-of-strategic-questions",
    "title": "4  Strategic Questions",
    "section": "4.4 Types of Strategic Questions",
    "text": "4.4 Types of Strategic Questions\nHill and Gerras provide three general types of strategic questions: definition, causations, and intervention questions (2020).\n\n4.4.1 Definition Questions\nThese clarify the nature of a problem. Definition questions establish a shared understanding of the environment and help scope the issue. There are three types of definition questions:\nDefine Nature: What is the thing we are analyzing and how is it interacting with the world around it?\n\n“What is the adversary trying to achieve?”\n“How serious is this challenge?”\n“What is Niger’s current policy towards the US?”\n\nDefine Extent: How big is the problem? What is the cost of inaction?\n\n“What is the operational reach of al Shabaab?”\n\nDefine Urgency: How is the problem unfolding in time?\n\n“How has the operational readiness levels of the East Africa Response Force changed in the past three years?”\n\n\n\n4.4.2 Causation Questions\nThese explore why something is happening and what the likely consequences will be.They help identify drivers, mechanisms, and emerging patterns. There are two types:\nExplanation: Why is it happening? What are the causes?\n\n“Why did the previous Nigerian government lack popular support?”\n\nPrediction: What is likely to happen because of this situation or event?\n\n“What kind of senior leaders is the current Somalian military personnel system likely to produce?”\n\n\n\n4.4.3 Intervention Questions\nAlso called “option” questions, these assess potential courses of action and their implications. These questions guide decision-making by linking means, ways, and ends under uncertainty. There are three types:\nEffectiveness: Does it work?\n\n“What is the likely effect of new sanctions on Sudan?”\n\nEfficiency: What is the relationship of benefits to costs?\n\n“What are the readiness improvements resulting from the change in design to African Lion? How do these improvements compare to the costs of this new design?”\n\nRobustness: Is the proposed intervention still sufficiently efficient or effective if we relax key assumptions?\n\n“How effective is our campaign plan if we lose access to bases in Country X?”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Strategic Questions</span>"
    ]
  },
  {
    "objectID": "04-strategic-questions.html#elements-of-good-and-bad-strategic-questions",
    "href": "04-strategic-questions.html#elements-of-good-and-bad-strategic-questions",
    "title": "4  Strategic Questions",
    "section": "4.5 Elements of good and bad strategic questions",
    "text": "4.5 Elements of good and bad strategic questions\n\n\n\n\n\n\n\n\nGood Strategic Questions\nBad Strategic Questions\nComments\n\n\n\n\nGrounded in the competitive context.\nDisplays little grounding in the context of the problem or issue.\nStrategic questions must relate to real-world competition—between states, actors, or systems. They should reflect the dynamics, interests, and intentions of adversaries or partners, rather than abstract or academic curiosity.\n\n\nHas two or more variables.\nVague regarding key variables.\nEffective strategic questions examine relationships between multiple factors (e.g., adversary intentions and regional stability). This encourages analysis of cause and effect, interdependencies, or trade-offs rather than isolated issues.\n\n\nStated clearly and unambiguously in question form.\nPresupposes the answer, includes the answer, or signals that only certain answers are acceptable.\nA good strategic question is direct, specific, and phrased as a question—not a vague statement or assumption. This ensures it can guide focused inquiry, analysis, and discussion.\n\n\nImplies the possibility of an observable answer.\nIncludes causal claims or solutions.\nStrategic questions should be answerable through available or collectible data. While they may not have immediate answers, they should point to evidence-based investigation or indicators.\n\n\nAcknowledges the uncertainty inherent in competition.\nIncludes moral or ethical claims or value statements that complicate quantification.\nRather than seeking certainty, good strategic questions embrace ambiguity and multiple plausible outcomes. They are designed to probe uncertainty, assess risk, and help navigate through complexity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Strategic Questions</span>"
    ]
  },
  {
    "objectID": "04-strategic-questions.html#practical-exercise",
    "href": "04-strategic-questions.html#practical-exercise",
    "title": "4  Strategic Questions",
    "section": "4.6 Practical Exercise",
    "text": "4.6 Practical Exercise\nDraft three strategic questions—one from each category—based on a current or historical conflict. Evaluate them for clarity, strategic relevance, and utility in informing decisions.\n\n4.6.1 Examples\n\n\n\n\n\n\n\nGood Questions\nBad Questions\n\n\n\n\nHow can we improve the protection of our forces against IED attacks?\nHow do we solve the IED problem?\n\n\nHow does incarceration affect the probability of a first-time offender’s future imprisonment?\nGiven that prisons are the higher education system of crime, how does incarceration affect the probability of a first-time offender’s future imprisonment?\n\n\nHow does the type of patrol (foot patrols vs. car patrols) affect the prevalence of violence in similar neighborhoods with otherwise similar military presences?\nHow does a U.S. military presence in Afghanistan affect violence in the country?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Strategic Questions</span>"
    ]
  },
  {
    "objectID": "04-strategic-questions.html#what-to-read",
    "href": "04-strategic-questions.html#what-to-read",
    "title": "4  Strategic Questions",
    "section": "4.7 What to Read",
    "text": "4.7 What to Read\n\nATP 5-0.3, Chapter II\nAsking Strategic Questions: A Primer for National Security Professionals\n\n\n\n\n\nCroft, Robert E. 2018. “Understanding Uncertainty: Incorporating the Unknown into Military Estimates.” U.S. Army War College. https://apps.dtic.mil/sti/pdfs/AD1069606.pdf.\n\n\nHill, Andrew, and Stephen Gerras. 2020. “Asking Strategic Questions: A Primer for National Security Professionals.” Joint Force Quarterly 96 (1): 37–41. https://ndupress.ndu.edu/Media/News/News-Article-View/Article/2076053/asking-strategic-questions-a-primer-for-national-security-professionals/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Strategic Questions</span>"
    ]
  },
  {
    "objectID": "05-strategic-military-effects.html",
    "href": "05-strategic-military-effects.html",
    "title": "5  Strategic Military Effects",
    "section": "",
    "text": "5.1 Motivating Problem\nA final theoretical piece of the military assessment puzzle focuses on strategic military effects. Analyzing military effects is a crucial topic that encompasses not only the debate over what constitutes an effect but also the key difference in how a military effect necessitates a distinct mode of thought, varying by the level of war.\nHow to think about and analyze strategic military effects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Strategic Military Effects</span>"
    ]
  },
  {
    "objectID": "05-strategic-military-effects.html#what-we-will-learn",
    "href": "05-strategic-military-effects.html#what-we-will-learn",
    "title": "5  Strategic Military Effects",
    "section": "5.2 What We Will Learn",
    "text": "5.2 What We Will Learn\n\nWhat are military effects?\nHow strategic military effects are different.\nA comparison of reductionist and systems thinking.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Strategic Military Effects</span>"
    ]
  },
  {
    "objectID": "05-strategic-military-effects.html#what-are-military-effects",
    "href": "05-strategic-military-effects.html#what-are-military-effects",
    "title": "5  Strategic Military Effects",
    "section": "5.3 What are Military Effects?",
    "text": "5.3 What are Military Effects?\nAccording to JP 3-0 (2022), military effects are:\n\nThe physical or behavioral state of a system that results from an action, a set of actions, or another effect.\nThe result, outcome, or consequence of an action.\nA change to a condition, behavior, or degree of freedom.\n\nThese effects can be direct, such as the destruction of an enemy capability, or indirect, such as influencing the will of a population. Effects can also be intended or unintended and may vary in scale, duration, and significance. Understanding military effects is critical for linking tactical actions to higher operational or strategic goals. At their core, effects represent a change in condition, behavior, or degree of freedom for an actor or system. This definition encompasses kinetic actions, like strikes, and non-kinetic actions, such as information operations or deterrent posturing. The comprehensive view ensures that planners consider both tangible and intangible results, enabling more effective integration of military activities with diplomatic, informational, and economic instruments of power in joint and combined operations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Strategic Military Effects</span>"
    ]
  },
  {
    "objectID": "05-strategic-military-effects.html#what-are-strategic-military-effects-and-why-are-they-different",
    "href": "05-strategic-military-effects.html#what-are-strategic-military-effects-and-why-are-they-different",
    "title": "5  Strategic Military Effects",
    "section": "5.4 What are Strategic Military Effects and Why are They Different?",
    "text": "5.4 What are Strategic Military Effects and Why are They Different?\nStrategic military effects are the broad, enduring outcomes that connect military actions to national or alliance-level objectives. They often span across domains, geographic regions, and timeframes, involving complex, multi-factor relationships beyond simple linear cause-and-effect. Unlike operational or tactical effects, strategic effects require consideration of political, economic, social, and informational dynamics alongside military factors. For example, a military show of force may have the immediate effect of deterring adversary aggression, but its strategic effect could include strengthening regional alliances or altering global perceptions of resolve. Because these effects operate within a highly interconnected environment, they are influenced by feedback loops, cultural factors, and adaptive adversary behavior. Understanding strategic effects requires analysts to look beyond battlefield outcomes and assess how actions contribute to—or detract from—long-term strategic objectives. This broader perspective ensures that military planning supports national policy and accounts for second- and third-order consequences.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Strategic Military Effects</span>"
    ]
  },
  {
    "objectID": "05-strategic-military-effects.html#reductionist-vs.-systems-thinking",
    "href": "05-strategic-military-effects.html#reductionist-vs.-systems-thinking",
    "title": "5  Strategic Military Effects",
    "section": "5.5 Reductionist vs. Systems Thinking",
    "text": "5.5 Reductionist vs. Systems Thinking\n\n5.5.1 Reductionist Thinking\nReductionist thinking is a mental model that simplifies problems into discrete, linear cause-and-effect relationships. It assumes predictability and determinism, making it well-suited for technical problem-solving and the development of tactical or operational plans where inputs and outcomes can be tightly controlled. In the military context, reductionist approaches are valuable for designing and executing tasks such as logistics scheduling, targeting plans, or force movements. To get a sense of the linear structure, Figure 5.1 shows the typical structure of how a military plan is organized. In theory, most planners assume that this structure provides a sound structure to assess a military campaign. In practice, however this is much more difficult.\n\n\n\n\n\n\nFigure 5.1: End State, Effects, Task List\n\n\n\nAt the strategic level, the reductionist model falls short because it often ignores the complexity of interconnected systems like the one shown in Figure 5.2. Strategic effects involve numerous interdependent variables—political, cultural, economic, and informational—that interact in unpredictable ways. Applying purely reductionist thinking to such problems risks oversimplification and flawed assumptions.\n\n\n\n\n\n\nFigure 5.2: Effects Network\n\n\n\nStrategic planning therefore benefits from additional tools, such as the factor–conditions matrix (Duczynski 2005) shown in Figure 5.3, which maps relationships between effects, means, and capabilities, and design/solution space analysis (Duczynski 2005) shown in Figure 5.4, which enables planners to explore multiple pathways and potential outcomes within a complex ecosystem.\n\n\n\n\n\n\nFigure 5.3: Factors Condition Matrix\n\n\n\n\n\n\n\n\n\nFigure 5.4: Effects and Means Comparison\n\n\n\n\n\n5.5.2 Systems Thinking\nSystems thinking offers a holistic approach to understanding and managing complexity by focusing on relationships, patterns, and interdependencies rather than isolating parts of a system like the image shown in Figure 5.5. In military strategy, it recognizes that actions in one area can trigger cascading effects in others, often in nonlinear and unexpected ways. This approach is especially suited to analyzing strategic military effects, where multiple actors, domains, and variables interact dynamically over time. Systems thinking encourages planners to account for feedback loops, time delays, and adaptive behavior—factors often overlooked in reductionist models. By viewing the operational environment as an interconnected whole, systems thinking enables a deeper appreciation of how military actions interact with political, economic, social, and informational systems. This perspective helps prevent tunnel vision, encourages creative solutions, and improves adaptability. Ultimately, systems thinking supports decision-makers in crafting strategies that are resilient, flexible, and responsive to the evolving realities of complex conflict environments.\n\n\n\n\n\n\nFigure 5.5: Systems Model",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Strategic Military Effects</span>"
    ]
  },
  {
    "objectID": "05-strategic-military-effects.html#practical-exercise",
    "href": "05-strategic-military-effects.html#practical-exercise",
    "title": "5  Strategic Military Effects",
    "section": "5.6 Practical Exercise",
    "text": "5.6 Practical Exercise\n\n5.6.1 Escalation Risk\nStrategic military effects become clearer when viewed through real-world examples. The Escalation Risk example illustrates how interactions between two actors’ activities, perceived threats, and resulting actions can create feedback loops that either intensify or de-escalate tensions. Strategic understanding here requires assessing not just immediate outcomes but the cumulative effects of reciprocal actions over time.\n\n\n\n\n\n\nFigure 5.6: Esclation Risk\n\n\n\n\n\n5.6.2 Overflight Agreement\nThe US–Algeria Agreement example shows how cooperative measures—such as document coordination and joint crisis response exercises—can build trust and confidence between nations. While the immediate operational effects may include improved interoperability or procedural alignment, the strategic effects could extend to stronger diplomatic ties and increased regional stability.\n\n\n\nUS-Algeria Agreement\n\n\nThese cases demonstrate that strategic effects are rarely the result of single, isolated actions. Instead, they emerge from patterns of interaction, shaped by perceptions, intentions, and broader context. By analyzing such examples, planners can better design actions that achieve desired long-term outcomes while managing risk.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Strategic Military Effects</span>"
    ]
  },
  {
    "objectID": "05-strategic-military-effects.html#what-to-read",
    "href": "05-strategic-military-effects.html#what-to-read",
    "title": "5  Strategic Military Effects",
    "section": "5.7 What to Read",
    "text": "5.7 What to Read\n\nATP 5-0.3, Chapter II\nIntroduction to Systems Thinking\n\n\n\n\n\nDuczynski, Guy. 2005. “Making Information Operations Effects-Based: Begin with the End (-State) in Mind!”\n\n\nJoint Chiefs of Staff. 2022. Joint Publication 3-0: Joint Campaigns and Operations. Washington, D.C.: United States Department of Defense. https://jdeis.js.mil/jdeis/new_pubs/jp3_0.pdf.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Strategic Military Effects</span>"
    ]
  },
  {
    "objectID": "06-developing-an-assessment-framework.html",
    "href": "06-developing-an-assessment-framework.html",
    "title": "6  Putting It All Together",
    "section": "",
    "text": "6.1 Motivating Problem\nFor this chapter, we will tie the theoretical information in the previous chapters with the principles discussed in ATP 5-0.3. The manual discusses six general questions that all assessment teams are trying to answer and communicate:\nAn effective operation assessment centers on the commander’s objectives, end state, and information requirements, integrating both quantitative and qualitative indicators. It interprets specific indicators in the broader operational context, informed by professional military judgment and subordinate unit capabilities. Analysis identifies trends, changes in the OE, and their operational impacts, drawing from the expertise of multiple staff sections and stakeholders.\nAssessments must be clear, concise, and contextually relevant—explaining why evidence and recommendations matter to achieving the end state. They measure progress against objectives, employ best practices such as standards-based assessments and theories of change, and produce actionable recommendations that make operations more effective.\nKey tenets include involving subordinate commanders, integrating assessments across all levels, embedding them into planning and battle rhythms, and incorporating external information sources for a holistic OE picture. Credibility and transparency require documenting methods, limitations, and assumptions. Assessment is continuous, adapting alongside planning and execution.\nIn general, the information in ATP 5-0.3 is sufficient for the operational and tactical levels. Using the information in the previous chapters will provide you the necessary building blocks for the strategic level, but can also supplement an assessment framework at any level.\nHow to develop an assessment framework depending on the level of your organization.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting It All Together</span>"
    ]
  },
  {
    "objectID": "06-developing-an-assessment-framework.html#what-we-will-learn",
    "href": "06-developing-an-assessment-framework.html#what-we-will-learn",
    "title": "6  Putting It All Together",
    "section": "6.2 What We Will Learn",
    "text": "6.2 What We Will Learn\n\nAssessment Planning\nData Collection and Management\nData Analysis\nCommunication and Presentation",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting It All Together</span>"
    ]
  },
  {
    "objectID": "06-developing-an-assessment-framework.html#assessment-planning",
    "href": "06-developing-an-assessment-framework.html#assessment-planning",
    "title": "6  Putting It All Together",
    "section": "6.3 Assessment Planning",
    "text": "6.3 Assessment Planning\n\n6.3.1 Using Logic Models and Futures Mapping to Build a Strategic Assessment Plan\nAt the strategic level, assessment requires more than tracking discrete metrics—it must link actions to desired national or theater outcomes while accounting for uncertainty and complexity. Two complementary tools—logic models and futures mapping—provide a structured way to design such assessments.\n\n\n6.3.2 Logic Models\nLogic models, discussed in Chapter 2, visualize the cause-and-effect pathway from resources and activities to strategic effects and end states. At their core, they map:\n\nInputs (resources, authorities, capabilities)\nActivities (operations, engagements, programs)\nOutputs (immediate deliverables)\nOutcomes (short- and intermediate-term changes in conditions or behavior)\nImpacts (long-term strategic effects)\n\nChapter II of ATP 5-0.3 (2020) emphasizes that, for strategic assessment, each link must be tested against assumptions, risks, and external influences. A logic model should clearly connect military activities to political and policy objectives, enabling decision-makers to visualize dependencies and identify where indicators and measures are needed. This helps define assessment questions, select relevant Measures of Performance (MOPs) and Measures of Effectiveness (MOEs), and ensure they support the commander’s decision requirements.\n\n\n6.3.3 Futures Mapping\nWhile logic models provide a linear causal framework, futures mapping, explained in Chapter 3, addresses uncertainty by visualizing alternative pathways the future might take. Drawing from systems thinking, it identifies key variables, drivers, and their potential interactions to produce multiple plausible scenarios. The process involves:\n\nIdentifying strategic objectives and desired end states.\nMapping drivers and influencing factors, including adversary actions, regional dynamics, and socio-political trends.\nExploring plausible futures by combining variables into different pathways (desired, undesired, neutral).\nIdentifying decision points and branch/sequel conditions that can shift the trajectory toward or away from objectives.\n\nIn practice, futures mapping can be integrated into the logic model’s “context” layer, helping planners recognize where non-linear changes or shocks might disrupt the causal chain. It ensures assessment plans do not assume a single deterministic path, but rather track indicators across multiple possible futures.\n\n\n6.3.4 Integration for Strategic Assessment\nA strategic assessment plan begins by building a logic model that aligns actions to strategic effects, then overlays futures mapping to identify uncertainty and potential divergence points. From this, planners derive: - Key assessment questions linked to decision-making needs. - Indicators and measures for both desired progress and early warning of negative trends. - Data collection requirements aligned with intelligence, partner reporting, and operational reporting.\nBy combining the structured causality of logic models with the adaptability of futures mapping, strategic assessments can better inform senior leaders, anticipate shifts in the environment, and adapt operations to sustain progress toward strategic objectives.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting It All Together</span>"
    ]
  },
  {
    "objectID": "06-developing-an-assessment-framework.html#data-collection-and-management",
    "href": "06-developing-an-assessment-framework.html#data-collection-and-management",
    "title": "6  Putting It All Together",
    "section": "6.4 Data Collection and Management",
    "text": "6.4 Data Collection and Management\n\n6.4.1 Data Collection and Management for Strategic-Level Assessment and Military Effects\nAt the strategic level, assessment hinges on the ability to connect data to the broader understanding of strategic military effects—the enduring outcomes that link military actions to national or alliance objectives. These effects often emerge from complex, multi-factor interactions, making the design of the data collection plan a critical enabler for accurate assessment.\n\n\n6.4.2 Planning Data Collection with Effects in Mind\nAccording to ATP 5-0.3, Chapter III, data requirements must be derived from the commander’s assessment framework, starting with strategic questions and logic models that explicitly connect activities to desired conditions. For strategic military effects—such as deterrence, alliance cohesion, or adversary capability degradation—planners must identify the key indicators that signal movement toward or away from these end states.\nThis process requires defining measures of performance (MOPs) to track the execution of tasks, and measures of effectiveness (MOEs) to gauge whether those tasks are producing the intended strategic effects. Data sources may include operational reporting, intelligence estimates, partner nation inputs, and even non-military indicators (e.g., economic or diplomatic trends).\n\n\n6.4.3 Data Management for Complex Effects\nStrategic military effects are rarely produced by a single action. They emerge from an ecosystem of diplomatic, informational, military, and economic activities. Data management must therefore integrate diverse datasets—across agencies, domains, and classification levels—into a unified structure. This involves: - Metadata tagging to link data points directly to specific effects or conditions. - Validation and verification to ensure accuracy and credibility. - Standardized formats for interoperability and cross-domain analysis.\nEffective management allows analysts to identify patterns, causal relationships, and unintended consequences. For example, tracking joint exercises with a partner nation should not only measure participation but also be linked to confidence-building indicators and regional deterrence metrics.\n\n\n6.4.4 Turning Data into Insight on Effects\nThe ultimate purpose of data collection is to inform whether strategic military effects are being achieved. Visualization tools, timelines, and geospatial overlays can help decision-makers see how conditions are shifting relative to desired end states. Importantly, assessment teams should revisit and refine indicators as the operational environment changes, ensuring continued relevance.\n\n\n6.4.5 Continuous Feedback Loop\nAs emphasized in ATP 5-0.3, the collection and management process must be iterative. Assessment findings may reveal that certain effects are not being realized, prompting adjustments in both operations and data priorities. This feedback loop ensures that the assessment remains anchored in the reality of the environment, rather than static assumptions.\nIn sum, effective strategic-level data collection and management is inseparable from understanding and measuring strategic military effects. By aligning collection plans with effect-based assessment frameworks, leaders can better evaluate progress toward strategic objectives, anticipate risks, and adapt campaigns to achieve enduring, favorable outcomes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting It All Together</span>"
    ]
  },
  {
    "objectID": "06-developing-an-assessment-framework.html#data-analysis",
    "href": "06-developing-an-assessment-framework.html#data-analysis",
    "title": "6  Putting It All Together",
    "section": "6.5 Data Analysis",
    "text": "6.5 Data Analysis\nEffective data analysis transforms collected and managed information into actionable insights that directly inform strategic decision-making. At the strategic level, this process must not only measure progress but also interpret complex causal relationships to understand why changes are occurring in the operational environment.\n\n6.5.1 Strategic-Level Considerations\nATP 5-0.3 emphasizes that analysis should focus on answering the commander’s assessment questions, evaluating trends over time, and determining whether progress is being made toward desired end states (2020). At the strategic level, this means interpreting indicators within the broader context of strategic military effects, where causal chains are long, multi-factorial, and often indirect.\nWhile doctrine such as JP 5-0 (2025) recommends the use of SMART objectives (Specific, Measurable, Achievable, Relevant, Time-bound) for assessment, this approach has limitations when applied to highly complex strategic objectives. Strategic goals often lack precise measurability, involve dynamic adversary behavior, and may span years or decades. As such, alternative frameworks can be more practical:\n\nOKR (Objectives and Key Results) – Focuses on setting ambitious objectives supported by flexible, outcome-oriented key results, which can adapt as the strategic environment changes (Doerr 2018).\nWOOP (Wish, Outcome, Obstacle, Plan) – Helps planners articulate desired conditions, anticipate obstacles, and create adaptive action plans, making it useful in contested and uncertain environments (Oettingen 2014).\n\n\n\n6.5.2 Analytical Methods\nGiven the complexity of strategic effects, social science methodologies can provide additional analytical rigor. One such method is process tracing (Collier 2011), a qualitative approach that examines the sequence of events and evidence linking activities to outcomes. This method is particularly useful for:\n\nTesting competing explanations for observed changes.\nIdentifying causal mechanisms underlying strategic effects.\nIntegrating qualitative and quantitative evidence to form a coherent narrative.\n\nProcess tracing can be combined with quantitative techniques such as regression, trend analysis, and network analysis to ensure findings are both empirically grounded and contextually rich.\n\n\n6.5.3 Integration into the Assessment Cycle\nData analysis at this level should:\n\nSynthesize inputs from diverse sources, ensuring cross-domain consistency. 2. Interpret trends in light of the operational and strategic context.\nTest hypotheses about causal pathways using both doctrinal and social science tools.\nInform recommendations that adjust operations, resource allocation, and engagement strategies.\n\nBy blending doctrinal guidance, adaptive planning frameworks, and rigorous analytical methods, strategic assessment teams can move beyond static measurement to dynamic understanding—enabling better anticipation of changes, more accurate evaluation of strategic effects, and more informed decision-making.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting It All Together</span>"
    ]
  },
  {
    "objectID": "06-developing-an-assessment-framework.html#communication-and-presentation",
    "href": "06-developing-an-assessment-framework.html#communication-and-presentation",
    "title": "6  Putting It All Together",
    "section": "6.6 Communication and Presentation",
    "text": "6.6 Communication and Presentation\nChapter IV of ATP 5-0.3 addresses Steps 5 and 6 of the operations assessment process: communicating the assessment and adapting the plan. The central idea is that even the most rigorous assessment is ineffective if it is not clearly conveyed in a way the commander understands and can act upon. The assessment is not the data or its visualization—it is the staff’s synthesized understanding of the operational environment (OE), why it is changing, and what should be done in response.\nCommunication must align with the commander’s decision-making style, battle rhythm, and information needs. Assessment products may include recommendations, OE condition updates, performance evaluations, and identification of risks or gaps. Effective communication tools range from written narratives (which provide depth and context) to visual aids such as stoplight charts, spider charts, and composite assessment products—each with clear standards to avoid misleading interpretations. Poor practices, such as “stoplights without standards” or “color math,” can undermine credibility.\nThe adaptation phase integrates the commander’s decisions back into planning and execution. Updates may occur via fragmentary orders, targeting cycles, or operational planning teams (OPTs). This step ensures that new OE insights are shared across the force and that plans remain relevant.\nUltimately, communication is iterative—commanders and staffs engage in dialogue to challenge assumptions, refine understanding, and maintain alignment between operations and strategic objectives, especially when supporting higher headquarters’ assessments.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting It All Together</span>"
    ]
  },
  {
    "objectID": "06-developing-an-assessment-framework.html#what-to-read",
    "href": "06-developing-an-assessment-framework.html#what-to-read",
    "title": "6  Putting It All Together",
    "section": "6.7 What to Read",
    "text": "6.7 What to Read\nATP 5-0.3\nSMART Objectives Paper by Doran\nObjectives and Key Results Methodology\nWOOP Methodology\nProcess Tracing Overview\n\n\n\n\nAir Land Sea Space Application Center. 2020. “Multi‑service Tactics, Techniques, and Procedures for Operation Assessment.” ATP 5‑0.3 / MCRP 5‑10.1 / NTTP 5‑01.3 / AFTTP 3‑2.87. Carlisle Barracks, PA: U.S. Department of Defense. https://www.alssa.mil/Portals/9/Documents/mttps/assessment_2020.pdf.\n\n\nCollier, David. 2011. Process Tracing: From Metaphor to Analytic Tool. Cambridge University Press.\n\n\nDoerr, John. 2018. Measure What Matters: OKRs: The Simple Idea That Drives 10x Growth. Portfolio.\n\n\nJoint Chiefs of Staff. 2025. Joint Publication 5-0: Joint Planning. Washington, D.C.: United States Department of Defense. https://jdeis.js.mil/jdeis/new_pubs/jp5_0.pdf.\n\n\nOettingen, Gabriele. 2014. Rethinking Positive Thinking: Inside the New Science of Motivation. Current.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting It All Together</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Air Land Sea Space Application Center. 2020. “Multi‑service\nTactics, Techniques, and Procedures for Operation Assessment.”\nATP 5‑0.3 / MCRP 5‑10.1 / NTTP 5‑01.3 / AFTTP 3‑2.87. Carlisle Barracks,\nPA: U.S. Department of Defense. https://www.alssa.mil/Portals/9/Documents/mttps/assessment_2020.pdf.\n\n\nCollier, David. 2011. Process Tracing: From Metaphor to Analytic\nTool. Cambridge University Press.\n\n\nCroft, Robert E. 2018. “Understanding Uncertainty: Incorporating\nthe Unknown into Military Estimates.” U.S. Army War College. https://apps.dtic.mil/sti/pdfs/AD1069606.pdf.\n\n\nDetroit Lions. 2025. “One Pride.” https://www.detroitlions.com/about-us/careers.\n\n\nDoerr, John. 2018. Measure What Matters: OKRs: The Simple Idea That\nDrives 10x Growth. Portfolio.\n\n\nDuczynski, Guy. 2005. “Making Information Operations\nEffects-Based: Begin with the End (-State) in Mind!”\n\n\nHickey, Christopher J, Robert Bradford, and Celestino Perez. 2019.\n“Design, Assessment, and the Causal Imperative in Complex\nOperations and Campaigns.” The Journal of Defense Modeling\nand Simulation 16 (4): 265–76.\n\n\nHill, Andrew, and Stephen Gerras. 2020. “Asking Strategic\nQuestions: A Primer for National Security Professionals.”\nJoint Force Quarterly 96 (1): 37–41. https://ndupress.ndu.edu/Media/News/News-Article-View/Article/2076053/asking-strategic-questions-a-primer-for-national-security-professionals/.\n\n\nHubbard, Douglas W. 2014. How to Measure Anything: Finding the Value\nof Intangibles in Business. John Wiley & Sons.\n\n\nJoint Chiefs of Staff. 2022. Joint Publication 3-0: Joint Campaigns\nand Operations. Washington, D.C.: United States Department of\nDefense. https://jdeis.js.mil/jdeis/new_pubs/jp3_0.pdf.\n\n\n———. 2025. Joint Publication 5-0: Joint Planning. Washington,\nD.C.: United States Department of Defense. https://jdeis.js.mil/jdeis/new_pubs/jp5_0.pdf.\n\n\nNational Football League.\nhttps://operations.nfl.com/inside-football-ops/nfl-operations/nfl-ops-honoring-the-game/.\n“NFL Ops: Honoring the Game.”\n\n\nOettingen, Gabriele. 2014. Rethinking Positive Thinking: Inside the\nNew Science of Motivation. Current.\n\n\nPaul, Christopher, and Miriam Matthews. 2018. “The Language of\nInform, Influence, and Persuade: Assessment Lexicon and Usage Guide for\nU.S. European Command Efforts.” RR-2655-EUCOM.\nSanta  Monica, CA: RAND Corporation. https://www.rand.org/pubs/research_reports/RR2655.html.\n\n\nRhyne, Russell. 1995. “Field Anomaly Relaxation: The Arts of\nUsage.” Futures 27 (6): 657–74.\n\n\nRhyne, Russell F. 1971. “Projecting Whole–Body Future Patterns–the\nField Anomaly Relaxation (FAR) Method. Memorandum Report.”\n\n\nW.K. Kellogg Foundation. 2004. “Logic Model Development\nGuide.” Battle Creek, MI: W.K. Kellogg Foundation. https://wkkf.issuelab.org/resource/logic-model-development-guide.html.",
    "crumbs": [
      "References"
    ]
  }
]