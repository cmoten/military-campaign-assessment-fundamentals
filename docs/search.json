[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Military Assessment Fundamentals",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html",
    "href": "01-campaign-assessment-overview.html",
    "title": "1  Military Assessment Overview",
    "section": "",
    "text": "1.1 Motivating Problem\nBoth Joint Publication 5-0 and Joint Publication 3-0 provide the following definitions for assessment:\nFrom these definitions, several key points emerge:\nHow to gain a better understanding of the analytic uncertainty inherent with military strategies, plans, and operations so that the commander and staff have a better understanding of the effectiveness with their operation or campaign (Croft 2018).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#what-we-will-learn",
    "href": "01-campaign-assessment-overview.html#what-we-will-learn",
    "title": "1  Military Assessment Overview",
    "section": "1.2 What We will learn",
    "text": "1.2 What We will learn\nInterestingly, neither publication offers a formal definition for measurement. To fill this conceptual gap, we apply Hubbard’s definition from the field of decision science: “A quantitatively expressed reduction in uncertainty based on one or more observations” (Hubbard 2014).\nThis framing aligns closely with the role assessment plays in military planning: reducing uncertainty to inform decisions about adapting or sustaining current operations.\nIn this chapter, we will explore the relationship between uncertainty and assessment and identify what types of assessment are appropriate at different levels of military operations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#assessment-as-a-prerequisite-to-adaptation",
    "href": "01-campaign-assessment-overview.html#assessment-as-a-prerequisite-to-adaptation",
    "title": "1  Military Assessment Overview",
    "section": "1.3 Assessment as a Prerequisite to Adaptation",
    "text": "1.3 Assessment as a Prerequisite to Adaptation\nCombatant Command Campaign Plans (CCPs) and country plans are dynamic, continuously evolving through cycles of implementation, evaluation, and revision. Because plans rarely unfold as anticipated, Combatant Command (CCMD) planners must extend their planning horizon annually to maintain a forward-looking posture. This dual imperative requires balancing the execution of ongoing operations with the need to plan for future contingencies.\nIn this environment, assessment becomes a prerequisite to effective adaptation. But assessment is not just about measuring performance or progress—it is fundamentally about measuring and reducing uncertainty. In military operations, uncertainty arises from the complexity of the operational environment, the unpredictability of adversaries, and the limitations of available information. By systematically observing and evaluating how operations affect outcomes, assessment helps commanders determine whether current approaches are likely to succeed—or whether they must be adjusted or abandoned.\nFollowing Hubbard’s definition, measurement is a quantitatively expressed reduction in uncertainty based on one or more observations (Hubbard 2014). This perspective is particularly relevant for campaign assessment: the goal is not only to track implementation but to reduce ambiguity about what works, what doesn’t, and why.\nThe following 13 propositions, summarized from contemporary scholarship (Hickey, Bradford, and Perez 2019), offer a theoretical framework for integrating uncertainty into military design and assessment:\n\nEnvironment as Obstacle Course – Military environments are composed of dynamic systems that commanders must navigate.\nEnvironment as Landscape to Change – Commanders are tasked not only with navigating but also with reshaping the environment.\nInterventions as Causal Hypotheses – Every military action implies a theory of cause and effect.\nNeed for Causal Literacy – Military professionals must be trained to craft and evaluate causal claims.\nTypology of Causal Logics – Reductive causation (structural, institutional, ideational, psychological) underpins military reasoning.\nMastering Causal Frameworks – Commanders must understand causal logics to assess and adapt effectively.\nComplexity Limits Certainty – The world’s complexity reduces the reliability of single-cause explanations.\nAssessment as Experimentation – Interventions should be approached as hypotheses tested through assessment.\nFocus on Political Ends – Military efforts must align with and support broader political objectives.\nReevaluate Measures of Effectiveness (MoEs) and Measures of Performance (MoPs) – Traditional measures of effectiveness and performance are insufficient for complex goals.\nAssessment is Difficult but Essential – Despite challenges, continuous assessment is indispensable.\nIntegration of Planning and Assessment – Planners and assessors should work collaboratively throughout the planning cycle.\n‘For the Sake Of’ vs. ‘In Order To’ Objectives – Objectives should prioritize purpose and direction over narrowly defined end-states.\n\nThese propositions highlight that uncertainty is not a problem to be eliminated, but a reality to be engaged thoughtfully. Assessment is not merely technical—it is cognitive, adaptive, and continuous. It helps leaders make decisions in complex and unpredictable environments, not by providing perfect knowledge, but by systematically reducing what is unknown.\nUltimately, campaign assessment enables the Combatant Commander (CCDR) to make strategic recommendations to the Secretary of Defense (SecDef) and broader Department of Defense (DoD) leadership. These insights guide both internal planning cycles and higher-level decisions about force structure, budgeting, and legislative requests through the Future Years Defense Program (FYDP).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#understanding-analytic-uncertainty",
    "href": "01-campaign-assessment-overview.html#understanding-analytic-uncertainty",
    "title": "1  Military Assessment Overview",
    "section": "1.4 Understanding Analytic Uncertainty",
    "text": "1.4 Understanding Analytic Uncertainty\nAccording to Croft (2018), analytic uncertainty is “a type of uncertainty that consists of five components of deep and surface uncertainty that is useful for planning and military assessments.” Table 1.1 lists those five elements which we will discuss in detail in the following section.\n\n\n\nTable 1.1: Elements of Analytic Uncertainty\n\n\n\n\n\nDeep Uncertainty (unknowables)\nSurface Uncertainty (unknowns)\n\n\n\n\nComplexity\nFactual Uncertainty\n\n\nNon-linearity\nPrecision Uncertainty\n\n\nAdaptation\n\n\n\n\n\n\n\n\n1.4.1 Components\n\n1.4.1.1 Surface Uncertainty\nSurface uncertainty refers to uncertainty that is visible and acknowledged, but not deeply understood. It often arises from ambiguities or contradictions in observable data or misalignment between facts and interpretation. Surface uncertainty is the “tip of the iceberg” — it’s apparent, but may conceal deeper issues beneath.\nSome key characteristics of surface uncertainty are it is:\n\nOften linked to conflicting or unclear indicators.\nMight be recognized by decision-makers but not resolved.\nCan result from overreliance on dashboards, over-simplified metrics, or data with unclear context.\n\nIf not properly mitigated, surface uncertainty can lead to:\n\nRisk of misinterpretation: Decision-makers may prematurely resolve uncertainty by defaulting to assumptions or bias.\nPlanning fragility: Plans built on surface-level observations without deeper validation may be brittle under stress.\nCognitive bias exposure: It invites confirmation bias and motivated reasoning when stakeholders pick evidence that supports preferred narratives.\n\nAs a mitigation measure, we will then learn more about the components of surface uncertainty: factual uncertainty and precision uncertainty.\n\n1.4.1.1.1 Factual Uncertainty\nFactual uncertainty arises when information is incomplete, inaccurate, outdated, or ambiguous. It’s about not having the right facts to begin with, and it reflects gaps in what we know or what we’re aware of. Some examples of this type of uncertainty are:\n\nMissing intelligence about an adversary’s troop movement.\nLack of updated terrain maps.\nAmbiguous language in diplomatic communication.\n\nBased on Figure 1.1, this form of uncertainty corresponds to known unknowns (things we are aware of but don’t understand) and unknown unknowns (things we are neither aware of nor understand).\n\n\n\n\n\n\nFigure 1.1: Factual Uncertainty\n\n\n\n\n\n1.4.1.1.2 Precision Uncertainty\nPrecision uncertainty refers to situations where we have quantitative information, but the information is imprecise or subject to variation. This often emerges when using probabilistic models or estimates that inherently involve a degree of variability or error.\nSome examples of this type of uncertainty are:\n\nForecasting casualty estimates with wide confidence intervals.\nUsing intelligence or other staff estimate reports that assign vague probabilities (e.g., “likely”).\nPlanning based on weather predictions with large error margins.\n\nIn practice, analysts in the above situations can use some of the models depicted in Figure 1.2, but these models generally are not utilized in assessment frameworks. A way to mitigate these risks in assessment frameworks would be to use techniques like Bayesian models and Monte Carlo simulation.\n\n\n\n\n\n\nFigure 1.2: Precision Uncertainty\n\n\n\n\n\n\n1.4.1.2 Deep Uncertainty\nDeep uncertainty arises when stakeholders do not know or cannot agree on:\n\nThe system model (how things work).\nThe probability distributions for key variables.\nThe value trade-offs involved in making decisions.\n\nUnlike surface uncertainty, which is visible but unclear, deep uncertainty means the underlying mechanisms are fundamentally opaque or contested. It’s common in complex, dynamic military, geopolitical, and socio-technical systems.\n\n1.4.1.2.1 Complexity\nThe operational environment consists of many interconnected parts whose relationships are not easily decomposed or predictable. Changes in one area can cause ripple effects elsewhere. Some examples of this component are:\n\nA change in local governance affects regional security, economy, and foreign relations.\nCoalition operations where each partner’s decisions affect the others in non-obvious ways.\n\nBy understanding this component, assessment analysts will begin to understand the difficulty in tracing cause and effect, and that emergent events are common even when they are not planned for (Air Land Sea Space Application Center 2020, 16, 21).\nFigure 1.3 depicts an important tool to mitigate for the previously discussed risks and that is systems thinking models. We will discuss these models more later in the course.\n\n\n\n\n\n\nFigure 1.3: Complexity\n\n\n\n\n\n1.4.1.2.2 Non-Linearity\nInputs and outputs are not proportional — small actions can have large effects or vice versa. The system behaves in non-predictable or chaotic ways at times.\nWhile it does not completely encapsulate this component, Figure 1.4 depicts a non-linear process. Other examples of this are:\n\nA minor political assassination triggers a large-scale war.\nLarge troop increases fail to shift insurgent activity.\n\nThis means that forecasting future events is highly error-prone and success or failure may hinge on tipping points or threshold effects. To mitigate these risks, an assessment analyst needs to utilize methods and tools that lend themselves to range estimates and sensitivity analysis.\n\n\n\n\n\n\nFigure 1.4: Non-Linearity\n\n\n\n\n\n1.4.1.2.3 Adaptation\nActors within the system — including adversaries, allies, civilians, and even the organization itself — observe, learn, and change behavior in response to actions.\nFor instance:\n\nAn adversary adjusts tactics after observing drone strikes.\nLocal populations change cooperation patterns based on perceived legitimacy.\n\nThis basically means the environment is dynamic and responsive, and the old adage of plans not making it past first contact still holds. A key mitigation for this concept is systems thinking models along with developing an agile assessment framework.\n\n\n\n\n1.4.2 Implication for Planning\nBy understanding the concept of surface uncertainty, assessment analysts can better define these components of uncertainty and ultimately measure it in a meaningful way. Analysts can also qualify risks and opportunities.\nSimilarly, understanding deep uncertainty at a conceptual level, provides us means to better deal with it. According to Croft (2018):\n\nThe remedy that analysts must employ in the face of complexity, nonlinearity and adaptation is theory. Theories, by their nature, are uncertain, and planners in early-stage operational design work must again avoid the assumption trap…By engaging with deep uncertainty, planners take on the challenge of building multiple worldviews as possible explanations of system behavior and change during conflict.\n\nThis means that instead of relying on the process of creating and validating assumptions, assessment analysts can now leverage additional analytic tools that make the engineering of assessment frameworks based on the previous thirteen principles possible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#from-uncertainty-to-theory",
    "href": "01-campaign-assessment-overview.html#from-uncertainty-to-theory",
    "title": "1  Military Assessment Overview",
    "section": "1.5 From Uncertainty to Theory",
    "text": "1.5 From Uncertainty to Theory\nGiven the previous explanations, we can now establish additional terms of reference based on the work of Hubbard (2014) and Croft (2018):\nAnalytic: Relating to or using analysis or logical reasoning.\nUncertainty: The lack of complete certainty, that is, the existence of more than one possibility. The “true” outcome/state/result/value is not known.\nRisk: A state of uncertainty where some of the possibilities involve a loss, catastrophe, or other undesired outcome.\nOpportunity: A state of uncertainty where some of the possibilities involve a favorable advance in effect, time to achievement, or other desired outcome if facilitated by decision, engagement, or resources.\nMeasurement: A quantitatively expressed reduction in uncertainty based on one or more observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#types-of-assessments-and-who-performs-them",
    "href": "01-campaign-assessment-overview.html#types-of-assessments-and-who-performs-them",
    "title": "1  Military Assessment Overview",
    "section": "1.6 Types of Assessments and Who Performs Them",
    "text": "1.6 Types of Assessments and Who Performs Them",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#linking-assessments-together",
    "href": "01-campaign-assessment-overview.html#linking-assessments-together",
    "title": "1  Military Assessment Overview",
    "section": "1.7 Linking Assessments Together",
    "text": "1.7 Linking Assessments Together",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "01-campaign-assessment-overview.html#practical-example",
    "href": "01-campaign-assessment-overview.html#practical-example",
    "title": "1  Military Assessment Overview",
    "section": "1.8 Practical Example",
    "text": "1.8 Practical Example\n\n\n\n\nAir Land Sea Space Application Center. 2020. “Multi‑service Tactics, Techniques, and Procedures for Operation Assessment.” ATP 5‑0.3 / MCRP 5‑10.1 / NTTP 5‑01.3 / AFTTP 3‑2.87. Carlisle Barracks, PA: U.S. Department of Defense. https://www.alssa.mil/Portals/9/Documents/mttps/assessment_2020.pdf.\n\n\nCroft, Robert E. 2018. “Understanding Uncertainty: Incorporating the Unknown into Military Estimates.” U.S. Army War College. https://apps.dtic.mil/sti/pdfs/AD1069606.pdf.\n\n\nHickey, Christopher J, Robert Bradford, and Celestino Perez. 2019. “Design, Assessment, and the Causal Imperative in Complex Operations and Campaigns.” The Journal of Defense Modeling and Simulation 16 (4): 265–76.\n\n\nHubbard, Douglas W. 2014. How to Measure Anything: Finding the Value of Intangibles in Business. John Wiley & Sons.\n\n\nJoint Chiefs of Staff. 2022. Joint Publication 3-0: Joint Campaigns and Operations. Washington, D.C.: United States Department of Defense. https://jdeis.js.mil/jdeis/new_pubs/jp3_0.pdf.\n\n\n———. 2025. Joint Publication 5-0: Joint Planning. Washington, D.C.: United States Department of Defense. https://jdeis.js.mil/jdeis/new_pubs/jp5_0.pdf.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Military Assessment Overview</span>"
    ]
  },
  {
    "objectID": "02-logic-models.html",
    "href": "02-logic-models.html",
    "title": "2  Logic Models",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Air Land Sea Space Application Center. 2020. “Multi‑service\nTactics, Techniques, and Procedures for Operation Assessment.”\nATP 5‑0.3 / MCRP 5‑10.1 / NTTP 5‑01.3 / AFTTP 3‑2.87. Carlisle Barracks,\nPA: U.S. Department of Defense. https://www.alssa.mil/Portals/9/Documents/mttps/assessment_2020.pdf.\n\n\nCroft, Robert E. 2018. “Understanding Uncertainty: Incorporating\nthe Unknown into Military Estimates.” U.S. Army War College. https://apps.dtic.mil/sti/pdfs/AD1069606.pdf.\n\n\nHickey, Christopher J, Robert Bradford, and Celestino Perez. 2019.\n“Design, Assessment, and the Causal Imperative in Complex\nOperations and Campaigns.” The Journal of Defense Modeling\nand Simulation 16 (4): 265–76.\n\n\nHubbard, Douglas W. 2014. How to Measure Anything: Finding the Value\nof Intangibles in Business. John Wiley & Sons.\n\n\nJoint Chiefs of Staff. 2022. Joint Publication 3-0: Joint Campaigns\nand Operations. Washington, D.C.: United States Department of\nDefense. https://jdeis.js.mil/jdeis/new_pubs/jp3_0.pdf.\n\n\n———. 2025. Joint Publication 5-0: Joint Planning. Washington,\nD.C.: United States Department of Defense. https://jdeis.js.mil/jdeis/new_pubs/jp5_0.pdf.",
    "crumbs": [
      "References"
    ]
  }
]